@article{jiang-etal-2022-fake,
title = {Fake news detection via knowledgeable prompt learning},
journal = {Information Processing & Management},
volume = {59},
number = {5},
pages = {103029},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103029},
url = {https://www.sciencedirect.com/science/article/pii/S030645732200139X},
author = {Gongyao Jiang and Shuang Liu and Yu Zhao and Yueheng Sun and Meishan Zhang},
keywords = {Fake news detection, Prompt learning, Pretrained language model, Knowledge utilization},
abstract = {The spread of fake news has become a significant social problem, drawing great concern for fake news detection (FND). Pretrained language models (PLMs), such as BERT and RoBERTa can benefit this task much, leading to state-of-the-art performance. The common paradigm of utilizing these PLMs is fine-tuning, in which a linear classification layer is built upon the well-initialized PLM network, resulting in an FND mode, and then the full model is tuned on a training corpus. Although great successes have been achieved, this paradigm still involves a significant gap between the language model pretraining and target task fine-tuning processes. Fortunately, prompt learning, a new alternative to PLM exploration, can handle the issue naturally, showing the potential for further performance improvements. To this end, we propose knowledgeable prompt learning (KPL) for this task. First, we apply prompt learning to FND, through designing one sophisticated prompt template and the corresponding verbal words carefully for the task. Second, we incorporate external knowledge into the prompt representation, making the representation more expressive to predict the verbal words. Experimental results on two benchmark datasets demonstrate that prompt learning is better than the baseline fine-tuning PLM utilization for FND and can outperform all previous representative methods. Our final knowledgeable model (i.e, KPL) can provide further improvements. In particular, it achieves an average increase of 3.28\% in F1 score under low-resource conditions compared with fine-tuning.}
}
